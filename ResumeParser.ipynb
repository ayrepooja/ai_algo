{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc5a0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "class constants:\n",
    "    # Omkar Pathak\n",
    "    NAME_PATTERN = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n",
    "\n",
    "    # Education (Upper Case Mandatory)\n",
    "    EDUCATION = [\n",
    "                'BE', 'B.E.', 'B.E', 'BS', 'B.S', 'ME', 'M.E',\n",
    "                'M.E.', 'MS', 'M.S', 'BTECH', 'MTECH',\n",
    "                'SSC', 'HSC', 'CBSE', 'ICSE', 'X', 'XII'\n",
    "            ]\n",
    "\n",
    "    NOT_ALPHA_NUMERIC = r'[^a-zA-Z\\d]'\n",
    "\n",
    "    NUMBER = r'\\d+'\n",
    "\n",
    "    # For finding date ranges\n",
    "    MONTHS_SHORT = r'''(jan)|(feb)|(mar)|(apr)|(may)|(jun)|(jul)\n",
    "                       |(aug)|(sep)|(oct)|(nov)|(dec)'''\n",
    "    MONTHS_LONG = r'''(january)|(february)|(march)|(april)|(may)|(june)|(july)|\n",
    "                       (august)|(september)|(october)|(november)|(december)'''\n",
    "    MONTH = r'(' + MONTHS_SHORT + r'|' + MONTHS_LONG + r')'\n",
    "    YEAR = r'(((20|19)(\\d{2})))'\n",
    "\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "    RESUME_SECTIONS_PROFESSIONAL = [\n",
    "                        'experience',\n",
    "                        'education',\n",
    "                        'interests',\n",
    "                        'professional experience',\n",
    "                        'publications',\n",
    "                        'skills',\n",
    "                        'certifications',\n",
    "                        'objective',\n",
    "                        'career objective',\n",
    "                        'summary',\n",
    "                        'leadership'\n",
    "                    ]\n",
    "\n",
    "    RESUME_SECTIONS_GRAD = [\n",
    "                        'accomplishments',\n",
    "                        'experience',\n",
    "                        'education',\n",
    "                        'interests',\n",
    "                        'projects',\n",
    "                        'professional experience',\n",
    "                        'publications',\n",
    "                        'skills',\n",
    "                        'certifications',\n",
    "                        'objective',\n",
    "                        'career objective',\n",
    "                        'summary',\n",
    "                        'leadership'\n",
    "                    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37df09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Omkar Pathak\n",
    "\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "from datetime import datetime\n",
    "from dateutil import relativedelta\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFSyntaxError\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cs = constants()\n",
    "\n",
    "class Utils():\n",
    "    def extract_text_from_pdf(pdf_path):\n",
    "        '''\n",
    "        Helper function to extract the plain text from .pdf files\n",
    "\n",
    "        :param pdf_path: path to PDF file to be extracted (remote or local)\n",
    "        :return: iterator of string of extracted text\n",
    "        '''\n",
    "        # https://www.blog.pythonlibrary.org/2018/05/03/exporting-data-from-pdfs-with-python/\n",
    "        if not isinstance(pdf_path, io.BytesIO):\n",
    "            # extract text from local pdf file\n",
    "            with open(pdf_path, 'rb') as fh:\n",
    "                try:\n",
    "                    for page in PDFPage.get_pages(\n",
    "                            fh,\n",
    "                            caching=True,\n",
    "                            check_extractable=True\n",
    "                    ):\n",
    "                        resource_manager = PDFResourceManager()\n",
    "                        fake_file_handle = io.StringIO()\n",
    "                        converter = TextConverter(\n",
    "                            resource_manager,\n",
    "                            fake_file_handle,\n",
    "                            codec='utf-8',\n",
    "                            laparams=LAParams()\n",
    "                        )\n",
    "                        page_interpreter = PDFPageInterpreter(\n",
    "                            resource_manager,\n",
    "                            converter\n",
    "                        )\n",
    "                        page_interpreter.process_page(page)\n",
    "\n",
    "                        text = fake_file_handle.getvalue()\n",
    "                        yield text\n",
    "\n",
    "                        # close open handles\n",
    "                        converter.close()\n",
    "                        fake_file_handle.close()\n",
    "                except PDFSyntaxError:\n",
    "                    return\n",
    "        else:\n",
    "            # extract text from remote pdf file\n",
    "            try:\n",
    "                for page in PDFPage.get_pages(\n",
    "                        pdf_path,\n",
    "                        caching=True,\n",
    "                        check_extractable=True\n",
    "                ):\n",
    "                    resource_manager = PDFResourceManager()\n",
    "                    fake_file_handle = io.StringIO()\n",
    "                    converter = TextConverter(\n",
    "                        resource_manager,\n",
    "                        fake_file_handle,\n",
    "                        codec='utf-8',\n",
    "                        laparams=LAParams()\n",
    "                    )\n",
    "                    page_interpreter = PDFPageInterpreter(\n",
    "                        resource_manager,\n",
    "                        converter\n",
    "                    )\n",
    "                    page_interpreter.process_page(page)\n",
    "\n",
    "                    text = fake_file_handle.getvalue()\n",
    "                    yield text\n",
    "\n",
    "                    # close open handles\n",
    "                    converter.close()\n",
    "                    fake_file_handle.close()\n",
    "            except PDFSyntaxError:\n",
    "                return\n",
    "\n",
    "\n",
    "    def get_number_of_pages(file_name):\n",
    "        try:\n",
    "            if isinstance(file_name, io.BytesIO):\n",
    "                # for remote pdf file\n",
    "                count = 0\n",
    "                for page in PDFPage.get_pages(\n",
    "                            file_name,\n",
    "                            caching=True,\n",
    "                            check_extractable=True\n",
    "                ):\n",
    "                    count += 1\n",
    "                return count\n",
    "            else:\n",
    "                # for local pdf file\n",
    "                if file_name.endswith('.pdf'):\n",
    "                    count = 0\n",
    "                    with open(file_name, 'rb') as fh:\n",
    "                        for page in PDFPage.get_pages(\n",
    "                                fh,\n",
    "                                caching=True,\n",
    "                                check_extractable=True\n",
    "                        ):\n",
    "                            count += 1\n",
    "                    return count\n",
    "                else:\n",
    "                    return None\n",
    "        except PDFSyntaxError:\n",
    "            return None\n",
    "\n",
    "\n",
    "    def extract_text_from_docx(doc_path):\n",
    "        '''\n",
    "        Helper function to extract plain text from .docx files\n",
    "\n",
    "        :param doc_path: path to .docx file to be extracted\n",
    "        :return: string of extracted text\n",
    "        '''\n",
    "        try:\n",
    "            temp = docx2txt.process(doc_path)\n",
    "            text = [line.replace('\\t', ' ') for line in temp.split('\\n') if line]\n",
    "            return ' '.join(text)\n",
    "        except KeyError:\n",
    "            return ' '\n",
    "\n",
    "\n",
    "    def extract_text_from_doc(doc_path):\n",
    "        '''\n",
    "        Helper function to extract plain text from .doc files\n",
    "\n",
    "        :param doc_path: path to .doc file to be extracted\n",
    "        :return: string of extracted text\n",
    "        '''\n",
    "        try:\n",
    "            try:\n",
    "                import textract\n",
    "            except ImportError:\n",
    "                return ' '\n",
    "            text = textract.process(doc_path).decode('utf-8')\n",
    "            return text\n",
    "        except KeyError:\n",
    "            return ' '\n",
    "\n",
    "\n",
    "    def extract_text(file_path, extension):\n",
    "        '''\n",
    "        Wrapper function to detect the file extension and call text\n",
    "        extraction function accordingly\n",
    "\n",
    "        :param file_path: path of file of which text is to be extracted\n",
    "        :param extension: extension of file `file_name`\n",
    "        '''\n",
    "        text = ''\n",
    "        if extension == '.pdf':\n",
    "            for page in extract_text_from_pdf(file_path):\n",
    "                text += ' ' + page\n",
    "        elif extension == '.docx':\n",
    "            text = extract_text_from_docx(file_path)\n",
    "        elif extension == '.doc':\n",
    "            text = extract_text_from_doc(file_path)\n",
    "        return text\n",
    "\n",
    "\n",
    "    def extract_entity_sections_grad(text):\n",
    "        '''\n",
    "        Helper function to extract all the raw text from sections of\n",
    "        resume specifically for graduates and undergraduates\n",
    "\n",
    "        :param text: Raw text of resume\n",
    "        :return: dictionary of entities\n",
    "        '''\n",
    "        text_split = [i.strip() for i in text.split('\\n')]\n",
    "        # sections_in_resume = [i for i in text_split if i.lower() in sections]\n",
    "        entities = {}\n",
    "        key = False\n",
    "        for phrase in text_split:\n",
    "            if len(phrase) == 1:\n",
    "                p_key = phrase\n",
    "            else:\n",
    "                p_key = set(phrase.lower().split()) & set(cs.RESUME_SECTIONS_GRAD)\n",
    "            try:\n",
    "                p_key = list(p_key)[0]\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if p_key in cs.RESUME_SECTIONS_GRAD:\n",
    "                entities[p_key] = []\n",
    "                key = p_key\n",
    "            elif key and phrase.strip():\n",
    "                entities[key].append(phrase)\n",
    "\n",
    "        # entity_key = False\n",
    "        # for entity in entities.keys():\n",
    "        #     sub_entities = {}\n",
    "        #     for entry in entities[entity]:\n",
    "        #         if u'\\u2022' not in entry:\n",
    "        #             sub_entities[entry] = []\n",
    "        #             entity_key = entry\n",
    "        #         elif entity_key:\n",
    "        #             sub_entities[entity_key].append(entry)\n",
    "        #     entities[entity] = sub_entities\n",
    "\n",
    "        # pprint.pprint(entities)\n",
    "\n",
    "        # make entities that are not found None\n",
    "        # for entity in cs.RESUME_SECTIONS:\n",
    "        #     if entity not in entities.keys():\n",
    "        #         entities[entity] = None\n",
    "        return entities\n",
    "\n",
    "\n",
    "    def extract_entities_wih_custom_model(custom_nlp_text):\n",
    "        '''\n",
    "        Helper function to extract different entities with custom\n",
    "        trained model using SpaCy's NER\n",
    "\n",
    "        :param custom_nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "        :return: dictionary of entities\n",
    "        '''\n",
    "        entities = {}\n",
    "        for ent in custom_nlp_text.ents:\n",
    "            if ent.label_ not in entities.keys():\n",
    "                entities[ent.label_] = [ent.text]\n",
    "            else:\n",
    "                entities[ent.label_].append(ent.text)\n",
    "        for key in entities.keys():\n",
    "            entities[key] = list(set(entities[key]))\n",
    "        return entities\n",
    "\n",
    "\n",
    "    def get_total_experience(experience_list):\n",
    "        '''\n",
    "        Wrapper function to extract total months of experience from a resume\n",
    "\n",
    "        :param experience_list: list of experience text extracted\n",
    "        :return: total months of experience\n",
    "        '''\n",
    "        exp_ = []\n",
    "        for line in experience_list:\n",
    "            experience = re.search(\n",
    "                r'(?P<fmonth>\\w+.\\d+)\\s*(\\D|to)\\s*(?P<smonth>\\w+.\\d+|present)',\n",
    "                line,\n",
    "                re.I\n",
    "            )\n",
    "            if experience:\n",
    "                exp_.append(experience.groups())\n",
    "        total_exp = sum(\n",
    "            [get_number_of_months_from_dates(i[0], i[2]) for i in exp_]\n",
    "        )\n",
    "        total_experience_in_months = total_exp\n",
    "        return total_experience_in_months\n",
    "\n",
    "\n",
    "    def get_number_of_months_from_dates(date1, date2):\n",
    "        '''\n",
    "        Helper function to extract total months of experience from a resume\n",
    "\n",
    "        :param date1: Starting date\n",
    "        :param date2: Ending date\n",
    "        :return: months of experience from date1 to date2\n",
    "        '''\n",
    "        if date2.lower() == 'present':\n",
    "            date2 = datetime.now().strftime('%b %Y')\n",
    "        try:\n",
    "            if len(date1.split()[0]) > 3:\n",
    "                date1 = date1.split()\n",
    "                date1 = date1[0][:3] + ' ' + date1[1]\n",
    "            if len(date2.split()[0]) > 3:\n",
    "                date2 = date2.split()\n",
    "                date2 = date2[0][:3] + ' ' + date2[1]\n",
    "        except IndexError:\n",
    "            return 0\n",
    "        try:\n",
    "            date1 = datetime.strptime(str(date1), '%b %Y')\n",
    "            date2 = datetime.strptime(str(date2), '%b %Y')\n",
    "            months_of_experience = relativedelta.relativedelta(date2, date1)\n",
    "            months_of_experience = (months_of_experience.years\n",
    "                                    * 12 + months_of_experience.months)\n",
    "        except ValueError:\n",
    "            return 0\n",
    "        return months_of_experience\n",
    "\n",
    "\n",
    "    def extract_entity_sections_professional(text):\n",
    "        '''\n",
    "        Helper function to extract all the raw text from sections of\n",
    "        resume specifically for professionals\n",
    "\n",
    "        :param text: Raw text of resume\n",
    "        :return: dictionary of entities\n",
    "        '''\n",
    "        text_split = [i.strip() for i in text.split('\\n')]\n",
    "        entities = {}\n",
    "        key = False\n",
    "        for phrase in text_split:\n",
    "            if len(phrase) == 1:\n",
    "                p_key = phrase\n",
    "            else:\n",
    "                p_key = set(phrase.lower().split()) \\\n",
    "                        & set(cs.RESUME_SECTIONS_PROFESSIONAL)\n",
    "            try:\n",
    "                p_key = list(p_key)[0]\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if p_key in cs.RESUME_SECTIONS_PROFESSIONAL:\n",
    "                entities[p_key] = []\n",
    "                key = p_key\n",
    "            elif key and phrase.strip():\n",
    "                entities[key].append(phrase)\n",
    "        return entities\n",
    "\n",
    "\n",
    "    def extract_email(text):\n",
    "        '''\n",
    "        Helper function to extract email id from text\n",
    "\n",
    "        :param text: plain text extracted from resume file\n",
    "        '''\n",
    "        email = re.findall(r\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", text)\n",
    "        if email:\n",
    "            try:\n",
    "                return email[0].split()[0].strip(';')\n",
    "            except IndexError:\n",
    "                return None\n",
    "\n",
    "\n",
    "    def extract_name(nlp_text, matcher):\n",
    "        '''\n",
    "        Helper function to extract name from spacy nlp text\n",
    "\n",
    "        :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "        :param matcher: object of `spacy.matcher.Matcher`\n",
    "        :return: string of full name\n",
    "        '''\n",
    "        pattern = [cs.NAME_PATTERN]\n",
    "\n",
    "        matcher.add('NAME', None, *pattern)\n",
    "\n",
    "        matches = matcher(nlp_text)\n",
    "\n",
    "        for _, start, end in matches:\n",
    "            span = nlp_text[start:end]\n",
    "            if 'name' not in span.text.lower():\n",
    "                return span.text\n",
    "\n",
    "\n",
    "    def extract_mobile_number(text, custom_regex=None):\n",
    "        '''\n",
    "        Helper function to extract mobile number from text\n",
    "\n",
    "        :param text: plain text extracted from resume file\n",
    "        :return: string of extracted mobile numbers\n",
    "        '''\n",
    "        # Found this complicated regex on :\n",
    "        # https://zapier.com/blog/extract-links-email-phone-regex/\n",
    "        # mob_num_regex = r'''(?:(?:\\+?([1-9]|[0-9][0-9]|\n",
    "        #     [0-9][0-9][0-9])\\s*(?:[.-]\\s*)?)?(?:\\(\\s*([2-9]1[02-9]|\n",
    "        #     [2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|([0-9][1-9]|\n",
    "        #     [0-9]1[02-9]|[2-9][02-8]1|\n",
    "        #     [2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?([2-9]1[02-9]|\n",
    "        #     [2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?([0-9]{7})\n",
    "        #     (?:\\s*(?:#|x\\.?|ext\\.?|\n",
    "        #     extension)\\s*(\\d+))?'''\n",
    "        if not custom_regex:\n",
    "            mob_num_regex = r'''(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\n",
    "                            [-\\.\\s]*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})'''\n",
    "            phone = re.findall(re.compile(mob_num_regex), text)\n",
    "        else:\n",
    "            phone = re.findall(re.compile(custom_regex), text)\n",
    "        if phone:\n",
    "            number = ''.join(phone[0])\n",
    "            return number\n",
    "\n",
    "\n",
    "    def extract_skills(nlp_text, noun_chunks, skills_file=None):\n",
    "        '''\n",
    "        Helper function to extract skills from spacy nlp text\n",
    "\n",
    "        :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "        :param noun_chunks: noun chunks extracted from nlp text\n",
    "        :return: list of skills extracted\n",
    "        '''\n",
    "        tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "        if not skills_file:\n",
    "            data = pd.read_csv(\n",
    "                os.path.join(os.path.dirname(__file__), 'skills.csv')\n",
    "            )\n",
    "        else:\n",
    "            data = pd.read_csv(skills_file)\n",
    "        skills = list(data.columns.values)\n",
    "        skillset = []\n",
    "        # check for one-grams\n",
    "        for token in tokens:\n",
    "            if token.lower() in skills:\n",
    "                skillset.append(token)\n",
    "\n",
    "        # check for bi-grams and tri-grams\n",
    "        for token in noun_chunks:\n",
    "            token = token.text.lower().strip()\n",
    "            if token in skills:\n",
    "                skillset.append(token)\n",
    "        return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n",
    "\n",
    "    def cleanup(token, lower=True):\n",
    "        if lower:\n",
    "            token = token.lower()\n",
    "        return token.strip()\n",
    "\n",
    "\n",
    "    def extract_education(nlp_text):\n",
    "        '''\n",
    "        Helper function to extract education from spacy nlp text\n",
    "\n",
    "        :param nlp_text: object of `spacy.tokens.doc.Doc`\n",
    "        :return: tuple of education degree and year if year if found\n",
    "                 else only returns education degree\n",
    "        '''\n",
    "        edu = {}\n",
    "        # Extract education degree\n",
    "        try:\n",
    "            for index, text in enumerate(nlp_text):\n",
    "                for tex in text.split():\n",
    "                    tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "                    if tex.upper() in cs.EDUCATION and tex not in cs.STOPWORDS:\n",
    "                        edu[tex] = text + nlp_text[index + 1]\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        # Extract year\n",
    "        education = []\n",
    "        for key in edu.keys():\n",
    "            year = re.search(re.compile(cs.YEAR), edu[key])\n",
    "            if year:\n",
    "                education.append((key, ''.join(year.group(0))))\n",
    "            else:\n",
    "                education.append(key)\n",
    "        return education\n",
    "\n",
    "\n",
    "    def extract_experience(resume_text):\n",
    "        '''\n",
    "        Helper function to extract experience from resume text\n",
    "\n",
    "        :param resume_text: Plain resume text\n",
    "        :return: list of experience\n",
    "        '''\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "\n",
    "        # word tokenization\n",
    "        word_tokens = nltk.word_tokenize(resume_text)\n",
    "\n",
    "        # remove stop words and lemmatize\n",
    "        filtered_sentence = [\n",
    "                w for w in word_tokens if w not\n",
    "                in stop_words and wordnet_lemmatizer.lemmatize(w)\n",
    "                not in stop_words\n",
    "            ]\n",
    "        sent = nltk.pos_tag(filtered_sentence)\n",
    "\n",
    "        # parse regex\n",
    "        cp = nltk.RegexpParser('P: {<NNP>+}')\n",
    "        cs = cp.parse(sent)\n",
    "\n",
    "        # for i in cs.subtrees(filter=lambda x: x.label() == 'P'):\n",
    "        #     print(i)\n",
    "\n",
    "        test = []\n",
    "\n",
    "        for vp in list(\n",
    "            cs.subtrees(filter=lambda x: x.label() == 'P')\n",
    "        ):\n",
    "            test.append(\" \".join([\n",
    "                i[0] for i in vp.leaves()\n",
    "                if len(vp.leaves()) >= 2])\n",
    "            )\n",
    "\n",
    "        # Search the word 'experience' in the chunk and\n",
    "        # then print out the text after it\n",
    "        x = [\n",
    "            x[x.lower().index('experience') + 10:]\n",
    "            for i, x in enumerate(test)\n",
    "            if x and 'experience' in x.lower()\n",
    "        ]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Omkar Pathak\n",
    "\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import io\n",
    "import spacy\n",
    "import pprint\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "utils = Utils()\n",
    "\n",
    "class ResumeParser(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        resume,\n",
    "        skills_file=None,\n",
    "        custom_regex=None\n",
    "    ):\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        custom_nlp = spacy.load(os.path.dirname(os.path.abspath(__file__)))\n",
    "        self.__skills_file = skills_file\n",
    "        self.__custom_regex = custom_regex\n",
    "        self.__matcher = Matcher(nlp.vocab)\n",
    "        self.__details = {\n",
    "            'name': None,\n",
    "            'email': None,\n",
    "            'mobile_number': None,\n",
    "            'skills': None,\n",
    "            'college_name': None,\n",
    "            'degree': None,\n",
    "            'designation': None,\n",
    "            'experience': None,\n",
    "            'company_names': None,\n",
    "            'no_of_pages': None,\n",
    "            'total_experience': None,\n",
    "        }\n",
    "        self.__resume = resume\n",
    "        if not isinstance(self.__resume, io.BytesIO):\n",
    "            ext = os.path.splitext(self.__resume)[1].split('.')[1]\n",
    "        else:\n",
    "            ext = self.__resume.name.split('.')[1]\n",
    "        self.__text_raw = utils.extract_text(self.__resume, '.' + ext)\n",
    "        self.__text = ' '.join(self.__text_raw.split())\n",
    "        self.__nlp = nlp(self.__text)\n",
    "        self.__custom_nlp = custom_nlp(self.__text_raw)\n",
    "        self.__noun_chunks = list(self.__nlp.noun_chunks)\n",
    "        self.__get_basic_details()\n",
    "\n",
    "    def get_extracted_data(self):\n",
    "        return self.__details\n",
    "\n",
    "    def __get_basic_details(self):\n",
    "        cust_ent = utils.extract_entities_wih_custom_model(\n",
    "                            self.__custom_nlp\n",
    "                        )\n",
    "        name = utils.extract_name(self.__nlp, matcher=self.__matcher)\n",
    "        email = utils.extract_email(self.__text)\n",
    "        mobile = utils.extract_mobile_number(self.__text, self.__custom_regex)\n",
    "        skills = utils.extract_skills(\n",
    "                    self.__nlp,\n",
    "                    self.__noun_chunks,\n",
    "                    self.__skills_file\n",
    "                )\n",
    "        # edu = utils.extract_education(\n",
    "        #               [sent.string.strip() for sent in self.__nlp.sents]\n",
    "        #       )\n",
    "        entities = utils.extract_entity_sections_grad(self.__text_raw)\n",
    "\n",
    "        # extract name\n",
    "        try:\n",
    "            self.__details['name'] = cust_ent['Name'][0]\n",
    "        except (IndexError, KeyError):\n",
    "            self.__details['name'] = name\n",
    "\n",
    "        # extract email\n",
    "        self.__details['email'] = email\n",
    "\n",
    "        # extract mobile number\n",
    "        self.__details['mobile_number'] = mobile\n",
    "\n",
    "        # extract skills\n",
    "        self.__details['skills'] = skills\n",
    "\n",
    "        # extract college name\n",
    "        try:\n",
    "            self.__details['college_name'] = entities['College Name']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        # extract education Degree\n",
    "        try:\n",
    "            self.__details['degree'] = cust_ent['Degree']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        # extract designation\n",
    "        try:\n",
    "            self.__details['designation'] = cust_ent['Designation']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        # extract company names\n",
    "        try:\n",
    "            self.__details['company_names'] = cust_ent['Companies worked at']\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            self.__details['experience'] = entities['experience']\n",
    "            try:\n",
    "                exp = round(\n",
    "                    utils.get_total_experience(entities['experience']) / 12,\n",
    "                    2\n",
    "                )\n",
    "                self.__details['total_experience'] = exp\n",
    "            except KeyError:\n",
    "                self.__details['total_experience'] = 0\n",
    "        except KeyError:\n",
    "            self.__details['total_experience'] = 0\n",
    "        self.__details['no_of_pages'] = utils.get_number_of_pages(\n",
    "                                            self.__resume\n",
    "                                        )\n",
    "        return\n",
    "\n",
    "\n",
    "def resume_result_wrapper(resume):\n",
    "    parser = ResumeParser(resume)\n",
    "    return parser.get_extracted_data()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     pool = mp.Pool(mp.cpu_count())\n",
    "\n",
    "#     resumes = []\n",
    "#     data = []\n",
    "#     for root, directories, filenames in os.walk('C:\\\\Users\\\\SAKSOFT093.SAKSOFT093PDC\\\\Documents\\\\pyresparser-master\\\\pyresparser-master\\\\resumes'):\n",
    "#         for filename in filenames:\n",
    "#             file = os.path.join(root, filename)\n",
    "#             resumes.append(file)\n",
    "\n",
    "#     results = [\n",
    "#         pool.apply_async(\n",
    "#             resume_result_wrapper,\n",
    "#             args=(x,)\n",
    "#         ) for x in resumes\n",
    "#     ]\n",
    "\n",
    "#     results = [p.get() for p in results]\n",
    "\n",
    "#     pprint.pprint(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyresparser import ResumeParser\n",
    "data = ResumeParser('C:\\\\Users\\\\SAKSOFT093.SAKSOFT093PDC\\\\Documents\\\\pyresparser-master\\\\pyresparser-master\\\\OmkarResume.pdf').get_extracted_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0706b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'college_name': None,\n",
    " 'company_names': None,\n",
    " 'degree': ['B.E. IN COMPUTER ENGINEERING'],\n",
    " 'designation': ['TECHNICAL CONTENT WRITER',\n",
    "                 'Machine Learning',\n",
    "                 'Schlumberger\\nDATA ENGINEER'],\n",
    " 'email': 'omkarpathak27@gmail.com',\n",
    " 'experience': ['Schlumberger',\n",
    "                'DATA ENGINEER',\n",
    "                'July 2018 - Present',\n",
    "                '• Responsible for implementing and managing an end-to-end '\n",
    "                'CI/CD Pipeline with custom validations for Informatica '\n",
    "                'migrations which',\n",
    "                'Pune, Maharashtra, India',\n",
    "                'brought migration time to 1.5 hours from 9 hours without any '\n",
    "                'manual intervention',\n",
    "                '• Enhancing, auditing and maintaining custom data ingestion '\n",
    "                'framework that ingest around 1TB of data each day to over 70 '\n",
    "                'business',\n",
    "                'units',\n",
    "                '• Working with L3 developer team to ensure the discussed '\n",
    "                'Scrum PBI’s are delivered on time for data ingestions',\n",
    "                '• Planning and Executing QA and Production Release Cycle '\n",
    "                'activities',\n",
    "                'Pune, Maharashtra, India',\n",
    "                'June 2018 - July 2018',\n",
    "                'Truso',\n",
    "                'FULL STACK DEVELOPER INTERN',\n",
    "                '• Created RESTful apis',\n",
    "                '• Tried my hands on Angular 5/6',\n",
    "                '• Was responsible for Django backend development',\n",
    "                'Propeluss',\n",
    "                'DATA ENGINEERING INTERN',\n",
    "                '• Wrote various automation scripts to scrape data from '\n",
    "                'various websites.',\n",
    "                '• Applied Natural Language Processing to articles scraped '\n",
    "                'from the internet to extract different entities in these '\n",
    "                'articles using entity',\n",
    "                'Pune, Maharashtra, India',\n",
    "                'October 2017 - January 2018',\n",
    "                'extraction algorithms and applying Machine Learning to '\n",
    "                'classify these articles.',\n",
    "                '• Also applied KNN with LSA for extracting relevant tags for '\n",
    "                'various startups based on their works.',\n",
    "                'GeeksForGeeks',\n",
    "                'TECHNICAL CONTENT WRITER',\n",
    "                '• Published 4 articles for the topics such as Data Structures '\n",
    "                'and Algorithms and Python',\n",
    "                'Pune, Maharashtra, India',\n",
    "                'July 2017 - September 2017',\n",
    "                'Softtestlab Technologies',\n",
    "                'WEB DEVELOPER INTERN',\n",
    "                '• Was responsible for creating an internal project for the '\n",
    "                'company using PHP and Laravel for testing purposes',\n",
    "                '• Worked on a live project for creating closure reports using '\n",
    "                'PHP and Excel',\n",
    "                'Pune, Maharashtra, India',\n",
    "                'June 2017 - July 2017'],\n",
    " 'mobile_number': '8087996634',\n",
    " 'name': '| \\uf0e0',\n",
    " 'no_of_pages': 3,\n",
    " 'skills': ['Php',\n",
    "            'Cloud',\n",
    "            'Reports',\n",
    "            'Linux',\n",
    "            'Training',\n",
    "            'Windows',\n",
    "            'Engineering',\n",
    "            'System',\n",
    "            'Parser',\n",
    "            'Content',\n",
    "            'Unix',\n",
    "            'Writing',\n",
    "            'Excel',\n",
    "            'Website',\n",
    "            'Machine learning',\n",
    "            'Github',\n",
    "            'Operating systems',\n",
    "            'Programming',\n",
    "            'Css',\n",
    "            'C',\n",
    "            'Api',\n",
    "            'Photography',\n",
    "            'Flask',\n",
    "            'Opencv',\n",
    "            'Scrum',\n",
    "            'Shell',\n",
    "            'Technical',\n",
    "            'Javascript',\n",
    "            'Django',\n",
    "            'Mysql',\n",
    "            'Testing',\n",
    "            'Python',\n",
    "            'Html',\n",
    "            'Migration',\n",
    "            'Security',\n",
    "            'Auditing',\n",
    "            'Algorithms',\n",
    "            'C++',\n",
    "            'Apis',\n",
    "            'Automation',\n",
    "            'Analytics'],\n",
    " 'total_experience': 4.0}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
